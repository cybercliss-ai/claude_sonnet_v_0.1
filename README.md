# claude_sonnet_v_0.1
# ğŸš€ **Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù…Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¥Ù„Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„**

## ğŸ“¥ **Ø§Ù„Ø¬Ø²Ø¡ 1: Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯**

### **1.1 Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø©**
```bash
# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
pip install transformers torch accelerate datasets peft bitsandbytes
pip install trl wandb sentencepiece protobuf

# Ù„Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
pip install flash-attn --no-build-isolation
```

### **1.2 ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ**
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    load_in_4bit=True,  # Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    trust_remote_code=True
)
```

## ğŸ”§ **Ø§Ù„Ø¬Ø²Ø¡ 2: Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬**

### **2.1 Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…**
```python
# ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ù„Ø¨ØªØ§Øª Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©
model = prepare_model_for_kbit_training(model)

# ØªÙƒÙˆÙŠÙ† LoRA Ù„Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙØ¹Ø§Ù„
lora_config = LoraConfig(
    r=16,  # Ø±ØªØ¨Ø© Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
    lora_alpha=32,
    target_modules=[
        "q_proj", "v_proj", "k_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# ØªØ·Ø¨ÙŠÙ‚ LoRA Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model = get_peft_model(model, lora_config)
```

### **2.2 Ø¥Ø¶Ø§ÙØ© Ø·Ø¨Ù‚Ø§Øª Ù…Ø®ØµØµØ©**
```python
import torch.nn as nn

class CustomAttentionLayer(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_size, 8)
        self.layer_norm = nn.LayerNorm(hidden_size)
    
    def forward(self, x):
        attended, _ = self.attention(x, x, x)
        return self.layer_norm(x + attended)

# Ø¥Ø¶Ø§ÙØ© Ø·Ø¨Ù‚Ø© Ù…Ø®ØµØµØ© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
def add_custom_layers(model, num_layers=2):
    for i in range(num_layers):
        custom_layer = CustomAttentionLayer(model.config.hidden_size)
        # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ØµØµØ© ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model.model.layers.insert(-1, custom_layer)
    return model

# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
model = add_custom_layers(model)
```

## ğŸ¯ **Ø§Ù„Ø¬Ø²Ø¡ 3: Ø·Ø±Ù‚ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©**

### **3.1 Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Ø± (Role System) Ù…Ø«Ù„ Claude**
```python
def create_claude_like_prompt(messages):
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø£Ø¯ÙˆØ§Ø± Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù€ Claude
    """
    system_prompt = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯. ÙŠØ¬Ø¨ Ø£Ù†:
    - ØªÙƒÙˆÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
    - ØªÙƒÙˆÙ† Ù…ØªÙˆØ§Ø²Ù†Ø§Ù‹ ÙÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯
    - ØªÙ‚Ø¯Ù… ØªÙØ³ÙŠØ±Ø§Øª Ù…Ù†Ø·Ù‚ÙŠØ©
    - ØªØ¹ØªØ±Ù Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªØ¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
    
    formatted = f"<|system|>\n{system_prompt}\n<|end|>\n"
    
    for msg in messages:
        if msg["role"] == "user":
            formatted += f"<|user|>\n{msg['content']}\n<|end|>\n"
        else:
            formatted += f"<|assistant|>\n{msg['content']}\n<|end|>\n"
    
    return formatted + "<|assistant|>\n"

# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…
test_messages = [
    {"role": "user", "content": "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ"}
]
prompt = create_claude_like_prompt(test_messages)
```

### **3.2 Ø¥Ø¶Ø§ÙØ© Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø³ÙŠØ§Ù‚**
```python
class ContextMemory:
    def __init__(self, max_tokens=4000):
        self.max_tokens = max_tokens
        self.conversation_history = []
    
    def add_interaction(self, user_input, assistant_response):
        self.conversation_history.append({
            "user": user_input,
            "assistant": assistant_response,
            "tokens": len(user_input.split()) + len(assistant_response.split())
        })
        self._trim_history()
    
    def _trim_history(self):
        total_tokens = sum(item["tokens"] for item in self.conversation_history)
        while total_tokens > self.max_tokens and self.conversation_history:
            removed = self.conversation_history.pop(0)
            total_tokens -= removed["tokens"]
    
    def get_context(self):
        context = ""
        for interaction in self.conversation_history[-5:]:  # Ø¢Ø®Ø± 5 ØªÙØ§Ø¹Ù„Ø§Øª
            context += f"User: {interaction['user']}\nAssistant: {interaction['assistant']}\n"
        return context

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
memory = ContextMemory()
```

## ğŸ“Š **Ø§Ù„Ø¬Ø²Ø¡ 4: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨**

### **4.1 ØªØ­Ø¶ÙŠØ± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª**
```python
from datasets import Dataset, load_dataset
import json

def load_training_data(file_path):
    """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ù…Ù„Ù JSON"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    formatted_data = []
    for item in data:
        # ØªÙ†Ø³ÙŠÙ‚ ÙƒÙ„ Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨
        prompt = create_claude_like_prompt([
            {"role": "user", "content": item["instruction"]}
        ])
        
        formatted_data.append({
            "text": prompt + item["output"] + tokenizer.eos_token,
            "prompt": prompt
        })
    
    return Dataset.from_list(formatted_data)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
dataset = load_training_data("training_data.json")
```

### **4.2 Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬**
```python
def tokenize_function(examples):
    """Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding=False,
        max_length=2048,
        return_offsets_mapping=False
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ labels (Ù†ÙØ³ input_ids Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ„ÙŠØ¯)
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized

# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
tokenized_dataset = dataset.map(tokenize_function, batched=True)
```

## ğŸ‹ï¸ **Ø§Ù„Ø¬Ø²Ø¡ 5: Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…**

### **5.1 Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©**
```python
training_args = TrainingArguments(
    output_dir="./claude-like-model",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=50,
    save_steps=500,
    eval_steps=500,
    evaluation_strategy="steps",
    save_total_limit=3,
    remove_unused_columns=False,
    push_to_hub=False,
    report_to="wandb",
    warmup_steps=100,
    lr_scheduler_type="cosine",
    weight_decay=0.01,
    optim="paged_adamw_32bit"
)
```

### **5.2 ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ callbacks Ù…Ø®ØµØµØ©**
```python
from transformers import TrainerCallback

class CustomCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            print(f"Step {state.global_step}: Loss = {logs.get('loss', 'N/A')}")
    
    def on_save(self, args, state, control, **kwargs):
        print(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ø®Ø·ÙˆØ© {state.global_step}")

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    callbacks=[CustomCallback()]
)

# Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
print("Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
trainer.train()
```

## ğŸ”„ **Ø§Ù„Ø¬Ø²Ø¡ 6: Ø·Ø±Ù‚ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©**

### **6.1 ØªØ¹Ø¯ÙŠÙ„ Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡**
```python
def modify_attention_mechanism(model):
    """ØªØ¹Ø¯ÙŠÙ„ Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    for layer in model.model.layers:
        # ØªØ¹Ø¯ÙŠÙ„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        original_attention = layer.self_attn
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù†ØªØ¨Ø§Ù‡ Ù…ØªÙ‚Ø§Ø·Ø¹
        layer.cross_attention = nn.MultiheadAttention(
            embed_dim=model.config.hidden_size,
            num_heads=model.config.num_attention_heads
        )
    
    return model

# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
model = modify_attention_mechanism(model)
```

### **6.2 Ø¥Ø¶Ø§ÙØ© Ø¢Ù„ÙŠØ© Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„**
```python
def add_chain_of_thought_capability(model, tokenizer):
    """Ø¥Ø¶Ø§ÙØ© Ù‚Ø¯Ø±Ø© Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„"""
    
    def chain_of_thought_generate(prompt, max_length=512):
        # Ø¥Ø¶Ø§ÙØ© ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„
        cot_prompt = prompt + "\nØ¯Ø¹Ù†Ø§ Ù†ÙÙƒØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:"
        
        inputs = tokenizer(cot_prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        return tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
    model.chain_of_thought_generate = chain_of_thought_generate
    
    return model

# ØªØ·Ø¨ÙŠÙ‚ Ø¢Ù„ÙŠØ© Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„
model = add_chain_of_thought_capability(model, tokenizer)
```

## ğŸ’¾ **Ø§Ù„Ø¬Ø²Ø¡ 7: Ø§Ù„Ø­ÙØ¸ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…**

### **7.1 Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨**
```python
# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
trainer.save_model("./claude-like-model-final")

# Ø­ÙØ¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª LoRA Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„
model.save_pretrained("./claude-like-lora-adapters")

# Ø­ÙØ¸ Ø§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ²Ø±
tokenizer.save_pretrained("./claude-like-model-final")
```

### **7.2 ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…**
```python
def load_trained_model(model_path):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    return model, tokenizer

# Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
model, tokenizer = load_trained_model("./claude-like-model-final")
```

## ğŸ§ª **Ø§Ù„Ø¬Ø²Ø¡ 8: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…**

### **8.1 Ø¯Ø§Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„Ø©**
```python
def test_model_comprehensive(model, tokenizer, test_cases):
    """Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
    results = []
    
    for i, test_case in enumerate(test_cases):
        prompt = create_claude_like_prompt([
            {"role": "user", "content": test_case}
        ])
        
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        results.append({
            "test_case": test_case,
            "response": response,
            "prompt": prompt
        })
        
        print(f"Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± {i+1}: {test_case}")
        print(f"Ø§Ù„Ø±Ø¯: {response}\n")
    
    return results

# Ø­Ø§Ù„Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù…ØªÙ†ÙˆØ¹Ø©
test_cases = [
    "Ø§Ø´Ø±Ø­ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø³Ø·",
    "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ø§Ù„ÙˆØ·Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØŸ",
    "Ø§ÙƒØªØ¨ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© Ø¹Ù† Ø±Ø­Ù„Ø© Ø¥Ù„Ù‰ Ø§Ù„ÙØ¶Ø§Ø¡"
]

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
results = test_model_comprehensive(model, tokenizer, test_cases)
```

## ğŸ“ **Ù†ØµØ§Ø¦Ø­ Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ù‡Ù…Ø©:**

1. **Ø§Ø¨Ø¯Ø£ ØµØºÙŠØ±Ø§Ù‹**: Ø¬Ø±Ø¨ Ù…Ø¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµØºÙŠØ±Ø© Ø£ÙˆÙ„Ø§Ù‹
2. **Ø§Ø­ØªÙØ¸ Ø¨Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©**: Ø§Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ ÙƒÙ„ Ù…Ø±Ø­Ù„Ø© ØªØ¯Ø±ÙŠØ¨ Ù…Ù‡Ù…Ø©
3. **Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡**: Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø¯ÙˆØ§Øª Ù…Ø«Ù„ wandb Ù„Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
4. **Ø§Ø®ØªØ¨Ø± Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±**: Ø§Ø®ØªØ¨Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ ÙƒÙ„ Ø¯ÙˆØ±Ø© ØªØ¯Ø±ÙŠØ¨
5. **Ø­Ø³Ù† Ø§Ù„ØªÙˆÙ‚ÙØ§Øª**: Ø§Ø³ØªØ®Ø¯Ù… Early Stopping Ù„ØªØ¬Ù†Ø¨ Overfitting

Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙŠØºØ·ÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©. Ø£ÙŠ Ø¬Ø²Ø¡ ØªØ±ÙŠØ¯ Ø£Ù† Ø£Ø±ÙƒØ² Ø¹Ù„ÙŠÙ‡ Ø£ÙƒØ«Ø± Ø£Ùˆ ØªØ´Ø±Ø­Ù‡ Ø¨ØªÙØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØŸ
